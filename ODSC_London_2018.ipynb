{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODSC London 22 September - Missing Data Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alexandru Agachi**, Empiric Capital\n",
    "\n",
    "m.a.agachi@gmail.com\n",
    "\n",
    "**Patric Fulop**, University of Edinburgh\n",
    "\n",
    "patric.fulop@gmail.com\n",
    "\n",
    "**Paul Pop**, Co Founder and CEO, Neurolabs\n",
    "\n",
    "paul@neurolabs.eu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But Why Missing Data??**\n",
    "\n",
    "We did not have to do much work with missing data in finance or machine vision. However, missing data became a real problem when dealing with clinical medical data, during our applications of machine learning to neuro oncogenetic data for survival prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/patricieni/ODSCLondon2018_MissingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most fields, missing data is so common that it is a given. You literally design your studies from the beginning with missing data in mind. The problem is particularly acute for surveys, longitudinal studies, and in clinical medicine.\n",
    "\n",
    "Overall, handling missing data is so important that Wainer (2010) considers it one of the “six necessary tools” that researchers need to master in order to successfully tackle problems in their fields in the 21st century.\n",
    "\n",
    "**Aim of this tutorial**\n",
    "1. To give you an understanding of missing data and the statistical problems it raises\n",
    "2. To provide you with a comprehensive, A to Z, applied tutorial on handling missing data\n",
    "3. Why Python pandas AND a**R**rrrrrh? Because Python/Pandas do not have the necessary libraries at this time.\n",
    "3. GitHub repository\n",
    "\n",
    "**How it's taught**\n",
    "\n",
    "1. Concepts and theory\n",
    "2. Code commentary\n",
    "3. Customer support\n",
    "\n",
    "patric.fulop@gmail.com\n",
    "\n",
    "paul@neurolabs.eu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data is a very difficult and niche topic. Brace yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is missing data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Values that are not available and that would be meaningful for analysis if they were observed\" (Little, 2012)**\n",
    "\n",
    "When you have missing data for variables that are not of interest in your study, you can consider that you do not have missing data. The first step is hence to reduce your dataset to the relevant variables. Only then you can start looking at missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Horse Dataset, with 30% missing values\n",
    "\n",
    "# Description http://archive.ics.uci.edu/ml/datasets/horse+colic\n",
    "\n",
    "# This is the path where the file is saved in our system\n",
    "path = \"data/horse.csv\"\n",
    "\n",
    "# We load the data into a pandas dataframe, and tell Pandas that the data is separated by commas\n",
    "horse_df = pd.read_csv(path, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "# Let's check the length of the dataset\n",
    "\n",
    "print(len(horse_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surgery</th>\n",
       "      <th>age</th>\n",
       "      <th>hospital_number</th>\n",
       "      <th>rectal_temp</th>\n",
       "      <th>pulse</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>temp_of_extremities</th>\n",
       "      <th>peripheral_pulse</th>\n",
       "      <th>mucous_membrane</th>\n",
       "      <th>capillary_refill_time</th>\n",
       "      <th>...</th>\n",
       "      <th>packed_cell_volume</th>\n",
       "      <th>total_protein</th>\n",
       "      <th>abdomo_appearance</th>\n",
       "      <th>abdomo_protein</th>\n",
       "      <th>outcome</th>\n",
       "      <th>surgical_lesion</th>\n",
       "      <th>lesion_1</th>\n",
       "      <th>lesion_2</th>\n",
       "      <th>lesion_3</th>\n",
       "      <th>cp_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>adult</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>cool</td>\n",
       "      <td>reduced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>more_3_sec</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>died</td>\n",
       "      <td>no</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  surgery    age  hospital_number  rectal_temp  pulse  respiratory_rate  \\\n",
       "0      no  adult           530101         38.5   66.0              28.0   \n",
       "\n",
       "  temp_of_extremities peripheral_pulse mucous_membrane capillary_refill_time  \\\n",
       "0                cool          reduced             NaN            more_3_sec   \n",
       "\n",
       "    ...    packed_cell_volume total_protein abdomo_appearance abdomo_protein  \\\n",
       "0   ...                  45.0           8.4               NaN            NaN   \n",
       "\n",
       "  outcome  surgical_lesion lesion_1 lesion_2  lesion_3  cp_data  \n",
       "0    died               no    11300        0         0       no  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first row to get an idea of what the data looks like\n",
    "\n",
    "horse_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_number</th>\n",
       "      <th>rectal_temp</th>\n",
       "      <th>pulse</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>nasogastric_reflux_ph</th>\n",
       "      <th>packed_cell_volume</th>\n",
       "      <th>total_protein</th>\n",
       "      <th>abdomo_protein</th>\n",
       "      <th>lesion_1</th>\n",
       "      <th>lesion_2</th>\n",
       "      <th>lesion_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.990000e+02</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.087733e+06</td>\n",
       "      <td>38.168619</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>30.460581</td>\n",
       "      <td>4.707547</td>\n",
       "      <td>46.307407</td>\n",
       "      <td>24.274436</td>\n",
       "      <td>3.039604</td>\n",
       "      <td>3659.709030</td>\n",
       "      <td>90.528428</td>\n",
       "      <td>7.387960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.532032e+06</td>\n",
       "      <td>0.733744</td>\n",
       "      <td>28.646219</td>\n",
       "      <td>17.666102</td>\n",
       "      <td>1.982311</td>\n",
       "      <td>10.436743</td>\n",
       "      <td>27.364194</td>\n",
       "      <td>1.967947</td>\n",
       "      <td>5408.472421</td>\n",
       "      <td>650.637139</td>\n",
       "      <td>127.749768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.184760e+05</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.289040e+05</td>\n",
       "      <td>37.800000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2111.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.303010e+05</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2322.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.347360e+05</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>56.750000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3209.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.305629e+06</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>41110.000000</td>\n",
       "      <td>7111.000000</td>\n",
       "      <td>2209.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hospital_number  rectal_temp       pulse  respiratory_rate  \\\n",
       "count     2.990000e+02   239.000000  275.000000        241.000000   \n",
       "mean      1.087733e+06    38.168619   72.000000         30.460581   \n",
       "std       1.532032e+06     0.733744   28.646219         17.666102   \n",
       "min       5.184760e+05    35.400000   30.000000          8.000000   \n",
       "25%       5.289040e+05    37.800000   48.000000         18.000000   \n",
       "50%       5.303010e+05    38.200000   64.000000         25.000000   \n",
       "75%       5.347360e+05    38.500000   88.000000         36.000000   \n",
       "max       5.305629e+06    40.800000  184.000000         96.000000   \n",
       "\n",
       "       nasogastric_reflux_ph  packed_cell_volume  total_protein  \\\n",
       "count              53.000000          270.000000     266.000000   \n",
       "mean                4.707547           46.307407      24.274436   \n",
       "std                 1.982311           10.436743      27.364194   \n",
       "min                 1.000000           23.000000       3.300000   \n",
       "25%                 3.000000           38.000000       6.500000   \n",
       "50%                 5.000000           45.000000       7.500000   \n",
       "75%                 6.500000           52.000000      56.750000   \n",
       "max                 7.500000           75.000000      89.000000   \n",
       "\n",
       "       abdomo_protein      lesion_1     lesion_2     lesion_3  \n",
       "count      101.000000    299.000000   299.000000   299.000000  \n",
       "mean         3.039604   3659.709030    90.528428     7.387960  \n",
       "std          1.967947   5408.472421   650.637139   127.749768  \n",
       "min          0.100000      0.000000     0.000000     0.000000  \n",
       "25%          2.000000   2111.500000     0.000000     0.000000  \n",
       "50%          2.300000   2322.000000     0.000000     0.000000  \n",
       "75%          3.900000   3209.000000     0.000000     0.000000  \n",
       "max         10.100000  41110.000000  7111.000000  2209.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can observe already from count that there's 58 values missing for respiratory_rate\n",
    "horse_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We picked up missing data (count row). Now what do we do?\n",
    "\n",
    "**Assessing how bad the situation is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surgery                    0\n",
       "age                        0\n",
       "hospital_number            0\n",
       "rectal_temp               60\n",
       "pulse                     24\n",
       "respiratory_rate          58\n",
       "temp_of_extremities       56\n",
       "peripheral_pulse          69\n",
       "mucous_membrane           47\n",
       "capillary_refill_time     32\n",
       "pain                      55\n",
       "peristalsis               44\n",
       "abdominal_distention      56\n",
       "nasogastric_tube         104\n",
       "nasogastric_reflux       106\n",
       "nasogastric_reflux_ph    246\n",
       "rectal_exam_feces        102\n",
       "abdomen                  118\n",
       "packed_cell_volume        29\n",
       "total_protein             33\n",
       "abdomo_appearance        165\n",
       "abdomo_protein           198\n",
       "outcome                    0\n",
       "surgical_lesion            0\n",
       "lesion_1                   0\n",
       "lesion_2                   0\n",
       "lesion_3                   0\n",
       "cp_data                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's actually compute the amount of missing data per variable in our dataset\n",
    "# pandas's isnull() function selects the null values in our dataset, and sum() adds them up per variable.\n",
    "\n",
    "horse_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization is extremely important, hence our focus on it throughout this tutorial. It is important for you in visualizing/assessing your missing data problem, it is important for communicating this to your audience later on, and it is important for discussing missing data with the domain experts/data collectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use missingno package to visualize missing data\n",
    "# Very niiiiice\n",
    "\n",
    "import missingno as msno\n",
    "msno.matrix(horse_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure some of these bivariate correlations = how is missingness expressed in our dataset if we take two variables at a time.\n",
    "\n",
    "R's VIM package allows us to quantify multivariate correlations as well, as we will see in the R notebook for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap of nullity correlations, a very important device to understand your pairwise missing data patterns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(horse_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These correlations provide more information about our missing data, than the univariate missing patterns (per each variables) alone.\n",
    "\n",
    "    1. -1 means that when one variable appears, the other definitely does not appear. \n",
    "    2. 0 means that variables have no influence on each other\n",
    "    3. +1 means that when one variables appears, the other most definitely appears. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the highest number is 0.8, **packed_cell_volume** and **total_protein** variables. So we would make the assumption that the protein total and cell volume are part of a similar clinical test, and when one was missed, the other one was too. \n",
    "\n",
    "On the other hand there is almost nothing <0, except **respiratory_rate** and **abdomen**, but that doesn't tell us much about the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes gives us the type of variable. Float and int relate to numbers of course, while object relates to non numbers\n",
    "horse_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect categorical vs numerical, simply for data exploration\n",
    "numerical = horse_df._get_numeric_data().columns\n",
    "categorical = list(set(horse_df.columns) - set(numerical))\n",
    "\n",
    "print(len(categorical),\" Categorical Columns are \\n\",categorical,'\\n')\n",
    "print(len(numerical),\"Numerical columns are \\n\",numerical)\n",
    "\n",
    "obj=horse_df[categorical]\n",
    "nonobj=horse_df[numerical]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will use **LabelEncoder** to transform the variables that contain categories represented as words (\"strings\") to be represented by categories that are numbers. \n",
    "\n",
    "We will treat missing values as the number **0**. Although we did do imputation by using fillna, this is just to ease our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform them using LabelEncoder without imputing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "clean_horse_df = horse_df.copy(deep=True)\n",
    "# Remove nan values in categories by using '0' (as a string)\n",
    "clean_horse_df[categorical] = clean_horse_df[categorical].fillna('0')\n",
    "# # Remove nan values in categories by using 0 as an int\n",
    "# clean_horse_df[numerical] = clean_horse_df[numerical].fillna(0)\n",
    "\n",
    "clean_horse_df[categorical] = clean_horse_df[categorical].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we check the different values that the temp_of_extremities variable can take\n",
    "horse_df.temp_of_extremities.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can observe that for the feature **temp_of_extremities** the word \"cool\" is encoded by 2 whereas \"normal\" by 3 and as mentioned previously, missing values \"NaN\" are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first five rows/observations for the variables temp_of_extremities. We see NaN values appear.\n",
    "horse_df['temp_of_extremities'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now printing first five rows/observations for the same variable, but from the cleaned/recoded data. NaN is now \"0\".\n",
    "clean_horse_df['temp_of_extremities'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the outcome variable in y\n",
    "y = clean_horse_df['outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have missing data. Why do I care?\n",
    "\n",
    "1. **Increase of bias/Underfitting**: \n",
    "    In statistics, bias reflects the extent to which your expected values differ from the true population parameters that you are trying to estimate. Concretely, bias arises in a dataset with missing data whenever for a variable, the missing data differs substantially from the observed data.\n",
    "\n",
    "    **Longitudinal study** and **dropout** : Let’s take a controlled drug study in medicine. In such a study, one compares the two arms of a study: the treatment arm and the control arm. Bias will depend on the relationship between missingness, treatment, and outcome. It is not uncommon for patients in the treatment arm to drop out due to adverse reactions to the treatment, or due to lack of improvement. In parallel, particularly healthy subjects, or ones who react exceptionally well to the treatment, will have very high completion rates for the study. The former, missing patients, will then become missing data in the final study. Ignoring them would lead to biased estimates of the efficacy of the treatment. We would be estimating how efficient the treatment was for a subsample of the population, the “healthier/better reacting” patients that completed the study, as opposed to the entire target population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Reduction of power:**\n",
    "    In statistics, power, scaled from 0 to 1, is the probability that a hypothesis test correctly rejects the null hypothesis when it is false. This is related to the probability of making a type 2 error i.e. failing to reject the null hypothesis when it is false. The higher the power of your study is, the less likely you are to make a type 2 error.\n",
    "\n",
    "Statistical power is influenced by two characteristics of your study: \n",
    "    1. Sample size \n",
    "    2. Variability of the outcomes observed. \n",
    "We therefore see that power increases if the sample size increases, or if the variability of the outcomes observed decreases. \n",
    "\n",
    "The mechanisms through which missing data directly influences the power of your study:\n",
    "    1. if you simply delete the observations with missing values, you reduce your effective sample size, and \n",
    "    therefore reduce your power\n",
    "    2. if your missing values are more extreme figures than the observed ones (outliers), which regularly happens in practice, you will underestimate your variability and therefore artificially narrow your confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Most statistical tools, theoretical and applied are designed for complete datasets**\n",
    "\n",
    "Almost all standard statistical analysis techniques and their implementations in various softwares were developed for complete datasets, and cannot handle appropriately incomplete ones (Schafer and Graham, 2002).\n",
    "\n",
    "(C/R) - Classification/Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Models| Handle Missing data| Reasoning\n",
    "---|---|---\n",
    "Linear Models (C/R) |No| Discriminative model, modelling of output rather than input\n",
    "Non-linear models: Neural Networks (C/R) |No| Same as above\n",
    "Decision Trees (C/R) |Yes| Defaults missing data to a node\n",
    "Random Forest (C/R) |Sometimes|Computationally intensive for growing trees. It also depends on the underlying algorithm, i.e. CART \n",
    "Bayesian Models & Generative Models |Yes| They can handle missing data probabilistically since you're modelling the input distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing data exploration. We inspect correlations in the dataset and correlation matrix in order to select items for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = clean_horse_df.corr()\n",
    "sns.heatmap(train_corr, vmax=0.8)\n",
    "corr_values = abs(train_corr['outcome']).sort_values(ascending=False)\n",
    "print(\"Correlation of features (X) wrt. outcome (Y) in ascending order:\\n\", corr_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Observe how respiratory rates are slightly higher for the ones who died \n",
    "2. Observe the peak around 0 for total proteins for the horses who died as well, suggesting total protein is an important measure for the outcome. \n",
    "3. Observe how the pulse is slightly lower for horses that lived and almost indistinguishable for horses that were dying or were euthanized..\n",
    "4. Notice how useful and pretty seaborn is, and if you haven't used it yet, now it's time to add it to your data science toolkit! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get a feel of the data and correlations. Coding of the variable \"outcome\": lived = 2, euthanized = 1, died = 0 \n",
    "g = sns.pairplot(data=clean_horse_df, \n",
    "                 vars=['packed_cell_volume','respiratory_rate',\n",
    "                       'rectal_temp','total_protein','pulse'], \n",
    "                 hue='outcome', \n",
    "                 height=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We established that, why and how missing data is a problem. Now what?\n",
    "\n",
    " If not convinced we'll redo everything up to here. So please be convinced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn to describe your missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Patterns of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Unit vs. item nonresponse** Unit nonresponse means all variables for an observation are missing i.e. a missing patient in a study. Item nonresponse means some variables for some observations are missing.\n",
    "2. **Univariate** The same observations having missing data across several variables.\n",
    "3. **Monotone vs. non-monotone** Monotone applies to longitudinal studies, when a patient drops out of a study for example at time t. All values after time t for that observation (patient) will be missing.\n",
    "4. **Arbitrary** Seemingly random pattern of missing values across the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/unit-nonresponse.png)\n",
    "![title](img/item-nonresponse.png)\n",
    "![title](img/univariate.png)\n",
    "![title](img/monotone.png)\n",
    "![title](img/non-monotone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanisms of missing data\n",
    "\n",
    "### How is the data missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. MCAR: Missing Completely at Random**\n",
    "\n",
    "If the missingness of the data is *unrelated* to both the observed and the unobserved data, the missing data is said to be missing completely at random (MCAR). In this situation, the missing data is a random subsample of the complete dataset. Discarding the observations with missing data will therefore not bias our estimates. In this specific case, analyzing only the observations with complete data in the study (complete case analysis) would lead to a loss of power/efficiency in the study, but not to a higher bias in the estimates. In practice, MCAR will rarely be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. MAR: Missing at Random** \n",
    "\n",
    "If missingness of the data depends only on the observed data and not on the unobserved data, we say that the data is missing at random (MAR). This implies that after taking the observed data into account, there are no systematic differences between items (subjects) with missing data and those without missing data. In practice, this MAR scenario is often the best we can hope for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. MNAR: Missing not at Random**\n",
    "This occurs when the missingness of the data depends on the values of the missing data themselves. It often occurs for the income variable in surveys for example: both high income earners and low income earners are less likely to report their income (to answer the income question) than are the average income earners. Therefore the fact that those answers will be missing will be correlated to the value of those missing data points themselves. The data in such a study cannot be said to be missing at random.\n",
    "\n",
    "\n",
    "Most statistical tools require data to be MAR or MCAR. However, in the presence of MNAR, the same statistical tools remain the best thing we have – the study in this case will simply need much more work with the domain experts/data collectors to understand the missingness of the data, and the results will be just much more uncertain/unreliable, meaning they will require extensive sensitivity analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now switching to the dark side: aRrrrrh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***But Sensei, if what I've done my whole life was wrong, what can I do now to atone for my sins?***\n",
    "\n",
    "Impute baby, impute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is imputation? \n",
    "\n",
    "What is imputation?\n",
    "\n",
    "Definition: Imputation amounts to filling in missing values with appropriate estimates for them, and then using standard complete data methods to study the now complete dataset. “From an operational standpoint, imputation solves the missing-data problem at the outset, enabling the analyst to proceed without further hindrance.” (Schafer, 1999)\n",
    "\n",
    "Objective: It is important to understand from the beginning the objective of imputation. It is not to estimate as accurately as possible any single missing value. The objective is to create a complete dataset that preserves as much as possible the characteristics of the original complete dataset. Intuitively this makes sense because in applied statistics, we aim to infer parameters about a population, based on the estimates computed from our sample. As such we are interested in statistics about aggregates, rather than in the exact value of single observations. As Little and Rubin (2002) state “It is important to note at the outset that usually sample surveys are conducted with the goal of making inferences about population quantities such as means, correlations and regression coefficients, and the values of individual cases in the data set are not the main interest. Thus, the objective of imputation is not to get the best possible predictions of the missing values, but to replace them by plausible values in order to exploit the information in the recorded variables in the incomplete cases for inference about population parameters.” \n",
    "\n",
    "Cautionary note: there are many methods to impute missing data. You need to be careful to both use one of the statistically robust methods, and to use a software package that implements it correctly. The theory is very esoteric and there are many permutations possible in implementing it. Choose mainstream multiple imputation methods and mainstream packages, per our recommendations at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single imputation\n",
    "As seen above already actually, in this \"technique,\" missing values are replaced with a single value derived from the non-missing values for each variable, for example the mean or mode of that variable in your dataset.\n",
    "\n",
    "This is (unfortunately) the most recommended method of imputation, especially in data analysis world. It is also the only method provided by frameworks such as pandas.\n",
    "\n",
    "What is the problem with single imputation? You are obviously not changing the mean nor mode of your variables since you are using it to fill in the missing values.\n",
    "\n",
    "Variances and covariances will be severely underestimated (Haitovsky, 1968), for two reasons. First, filling in all missing values with the mean will not account for the variation that would most likely be present in reality between those observed values. You are imputing the mean for every missing value, while the real values would probably vary around the mean value. Second, your ultimately increased sample size will result in smaller standard errors, and these will not accurately reflect the uncertainty actually existing in your dataset due to those missing values. In studying single imputation methods, Pigott (2001) concludes that “under no circumstances does mean imputation produce unbiased results…Bias in the estimation of variances and standard errors are compounded when estimating multivariate parameters such as regression coefficients.”\n",
    "\n",
    "What about longitudinal studies and the common practice of LOCF (last observation carried forward) and BOCF (baseline observation carried forward)?\n",
    "\n",
    "They are simply specific cases of single imputation. As such, they do not take into account the information about the missing data contained in the observed data, they lead to biased estimates, and they underestimate the uncertainty contained in the dataset.\n",
    "\n",
    "Let's see this for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will pick two variables that have missing values to perform a regression on the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of missing values for nasogastric_reflux_ph: %d\" %clean_horse_df.nasogastric_reflux_ph.isnull().sum())\n",
    "print(\"Number of missing values for nasogastric_reflux_: %d\" %(clean_horse_df.nasogastric_reflux==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_horse_1 = clean_horse_df.copy(deep=True)\n",
    "clean_horse_1 = clean_horse_1[['nasogastric_reflux_ph','nasogastric_reflux','outcome']]\n",
    "clean_horse_1 = clean_horse_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_horse_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(data = clean_horse_1, \n",
    "                 vars=['nasogastric_reflux_ph','nasogastric_reflux'],\n",
    "                 hue='outcome',\n",
    "                 height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before single imputation, the **nasogastric_reflux_ph** is higher for the dead animals (top left). However, once single imputation has been performed with the mean, we observe a complete overlap between dead and alive (Blue/green) (next figure top left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_horse_1.nasogastric_reflux_ph.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_horse_1.nasogastric_reflux_ph.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairplot(dataframe, fill_na_mode):\n",
    "    temp_df = dataframe.copy(deep=True)\n",
    "\n",
    "    temp_df.nasogastric_reflux_ph.fillna(getattr(temp_df.nasogastric_reflux_ph, fill_na_mode)(),inplace=True) \n",
    "    temp_df.nasogastric_reflux.fillna(temp_df.nasogastric_reflux.mode(),inplace=True)\n",
    "    g = sns.pairplot(data = temp_df, \n",
    "                     vars=['nasogastric_reflux_ph','nasogastric_reflux'],\n",
    "                     hue='outcome',\n",
    "                     height=3)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pairplot(clean_horse_df, 'mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pairplot(clean_horse_df, 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try imputing **nasogastric_reflux_ph** with the **median**! They are even more confounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pairplot(clean_horse_df, 'median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing with the mode of the data, we observe the distributions are somewhat preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pairplot(clean_horse_df, 'mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear estimator analysis\n",
    "The bias for the linear estimator will increase if the missing data is actually not around the mean. To demonstrate this we will compare the Bias by imputing with the mean and median.\n",
    "\n",
    "The more different the actual missing data is from the mean, the larger the bias will be. For illustration purposes, we will implement a regression model dependent on two variables: packed_cell_volume and pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "X = clean_horse_df[['packed_cell_volume','pulse','outcome']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Complete-case analysis: Remove missing values**\n",
    "\n",
    "Complete case analysis amounts to deleting all observations for which we have missing data. Very commonly done in practice.\n",
    "\n",
    "If the data is missing completely at random (MCAR), then complete case analysis can be considered (Nakai & Weiming, 2011; Allison, 2001). In this case your reduced dataset will be a random subsample of the full dataset – the parameter estimates therefore will be as unbiased for the reduced dataset as they would have been for the full dataset. \n",
    "\n",
    "Allison (2001) also finds that it is more robust than all other non robust methods, such as available case analysis.\n",
    "\n",
    "Unfortunately, complete case analysis has been shown to produce biased estimates if the data is not missing completely at random (Bell et al, 2013), in addition to the loss of information and power that result from all the observations hence deleted. This is why you should not do this in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Baseline\" regression with non-imputed dataset, removing features that miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_non_imputed = X.dropna()\n",
    "y_non_imputed = X_non_imputed.outcome\n",
    "X_non_imputed = X_non_imputed.drop('outcome',axis=1)\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_non_imputed, y_non_imputed)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_non_imputed)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_non_imputed, y_pred))\n",
    "# Explained variance score: 1 is perfect p|rediction\n",
    "print('Variance score: %.2f' % r2_score(y_non_imputed, y_pred))\n",
    "val1 = mean_squared_error(y_non_imputed, y_pred) - np.var(y_pred)\n",
    "print('Bias squared: %.2f' % val1)\n",
    "r1 = regr.coef_\n",
    "var1 = np.var(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Available Case Analysis: Impute features using their mean**\n",
    "\n",
    "Available case analysis amounts to taking your incomplete dataset with missing values, and for each variable, using all the observations that do not have missing values for that variable. In practice this means that in the same study you will use different subsamples from your dataset for different variables of interest.\n",
    "\n",
    "Whaaaaat?\n",
    "\n",
    "How is this possible? In linear regression we know that a regression can be estimated using only either the sample means and covariance matrix, or the means, standard deviations and correlation matrix (see any introductory statistical book such as Introduction to Statistical Learning). Pairwise deletion aims to take advantage of this insight, computing these statistics using all the cases for which data is available, across our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a linear regression on the imputed data and look at the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(dataset, fill_na_method, offset=0):\n",
    "    X_imputed = dataset[['packed_cell_volume','pulse']]\n",
    "    X_imputed['packed_cell_volume'] = X_imputed['packed_cell_volume'].fillna(\n",
    "        getattr(X_imputed['packed_cell_volume'], fill_na_method)() - offset)\n",
    "    X_imputed['pulse'] = X_imputed['pulse'].fillna(\n",
    "        getattr(X_imputed['pulse'], fill_na_method)() - offset)\n",
    "    y_imputed = y \n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_imputed, y_imputed)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_imputed)\n",
    "\n",
    "    # The coefficients\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_imputed, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance explained: %.2f' % r2_score(y_imputed, y_pred))\n",
    "    val2 = mean_squared_error(y_imputed, y_pred) - np.var(y_pred)\n",
    "    print('Bias squared: %.2f' % val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_regression(clean_horse_df, 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Impute using \"outliers\" or data that is far below the median.**\n",
    "\n",
    "For illustration purposes we picked median-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_regression(clean_horse_df, 'median', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Imputing with predictions**\n",
    "\n",
    "i.e. Classifying outcome with ... for example logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 2 variables and \"predict\" 20% of the outcomes. \n",
    "# For illustration purposes\n",
    "X_imputed = clean_horse_df[['packed_cell_volume','pulse','outcome']]\n",
    "X_imputed['packed_cell_volume'] = X_imputed['packed_cell_volume'].fillna(X_imputed['packed_cell_volume'].mean())\n",
    "X_imputed['pulse'] = X_imputed['pulse'].fillna(X_imputed['pulse'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fixed numpy seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "msk = np.random.rand(len(X_imputed)) < 0.8\n",
    "\n",
    "data_train = X_imputed[msk]\n",
    "data_test = X_imputed[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "X_train = data_train[['packed_cell_volume','pulse']]\n",
    "Y_train = data_train[['outcome']]\n",
    "\n",
    "X_test = data_test[['packed_cell_volume','pulse']]\n",
    "Y_test = data_test[['outcome']]\n",
    "lr_model.fit(X_train,Y_train.values.ravel())\n",
    "\n",
    "predictions = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect predictions vs. original data\n",
    "predictions - Y_test.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now back to R!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's finish with a bang not a whisper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubin's pooling rules for 10 different dataset imputations.\n",
    "\n",
    "Multiple imputation amounts to generating a desired number of complete datasets, and of then running our analyses on each dataset. However, we need one single result at the end of our analysis. This is what Rubin's rules do: they allow us to combine, in a statistically robust way, the results of our analysis across all our imputed datasets, in order to arrive at one single result.\n",
    "\n",
    "Do not average the multiply imputed data and then analyze the averages as if it were complete! Why? This would result in incorrect standard errors, confidence intervals, and p values. It would present the same drawbacks as single imputation.\n",
    "\n",
    "Rubin's rules were first described in his 1987 textbook on handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this the Correct version?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example below, after multiple imputation, we have 10 datasets that we want to average to find the best \n",
    "imputation together with the associated variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_estimates = np.zeros((10,1))\n",
    "data_estimates_var = np.zeros((10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10): \n",
    "    X_imputed = clean_horse_df[['packed_cell_volume','pulse']]    \n",
    "    \n",
    "    # Fill missing values with random numbers but make sure they are positive.\n",
    "    # We make the assumption of independence between the two variables. \n",
    "    imputation_volume = np.abs(X_imputed['packed_cell_volume'].mean() - np.random.randint(0,30))\n",
    "    X_imputed['packed_cell_volume'] = X_imputed['packed_cell_volume'].fillna(imputation_volume)\n",
    "    \n",
    "    imputation_pulse = np.abs(X_imputed['pulse'].median()-np.random.randint(0,30))\n",
    "    X_imputed['pulse'] = X_imputed['pulse'].fillna(imputation_pulse)\n",
    "    y_imputed = y\n",
    "    # The correct estimates \n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_imputed, y_imputed)\n",
    "\n",
    "    # Make predictions - these are the estimates\n",
    "    y_pred = regr.predict(X_imputed)\n",
    "\n",
    "    weights = regr.coef_\n",
    "    variance = mean_squared_error(y_imputed, y_pred)\n",
    "    r2 = r2_score(y_imputed, y_pred)\n",
    "    #bias = mean_squared_error(y_imputed, y_pred) - np.var(y_pred)\n",
    "    \n",
    "    # The coefficients of regression are the estimates. \n",
    "    # Could use either\n",
    "    mean_estimates[i,:] = y_pred.mean()\n",
    "    #mean_estimates[i,:] = y_pred\n",
    "    \n",
    "    # Covariance of 2D dataset\n",
    "    # Covariance structure - (var(x,x), var(x,y), var(y,x),var(y,y))\n",
    "    data_estimates_var[i,:] = variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one mean and covariance Q_l and U_l\n",
    "print(\"Means (Estimates): \\n\",mean_estimates[1])\n",
    "print(\"\\nVariance of estimates:\\n\",data_estimates_var[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto Rubin's rules\n",
    "\n",
    "Datasets: $m = 10$  \n",
    "1. Overall mean for data estimates: $\\hat{Q} = \\sum_{l=1}^{m}\\hat{Q_l}$  \n",
    "2. Variance-Covariance for data estimates based on covariance of each estimate: $\\hat{U} = \\sum_{l=1}^{m}\\hat{U_l}$\n",
    "\n",
    "3. Variance in the estimates: $B = \\frac{1}{m-1} \\sum_{l=1}^{m}(\\hat{Q_l}-\\hat{Q})^T * (\\hat{Q_l}-\\hat{Q})$\n",
    "4. Total variance for real $Q$: $Q-\\hat{Q}: T = \\hat{U} + B + \\frac{B}{m} $  \n",
    "5. Relative increase in variance due to nonresponse: $r = (1 + \\frac{1}{m})*\\frac{B}{\\hat{U}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Overall means \n",
    "Q_hat = mean_estimates.mean(axis=0)\n",
    "print(\"Overall Means:\",Q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Covariance\n",
    "U_hat = data_estimates_var.mean(axis=0)\n",
    "print(\"Overall mean variance in datasets\\n\",U_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Variance in estimates - take 1\n",
    "diff = Q_hat-mean_estimates\n",
    "B1 = 1/10 * np.dot(diff.T,diff)\n",
    "print(\"Take 1: Variance within the estimates\\n\",B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check yourself before you wreck yourself\n",
    "#3. Variance in estimates - take 2 \n",
    "B2 = np.var(mean_estimates.T) \n",
    "print(\"Take 1: Variance within the estimates\\n\",B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(B1 == B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Total variance\n",
    "T = U_hat + (1+1/10)*B1\n",
    "print(\"Total Variance\\n\",T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Relative increase in variance due to nonresponse\n",
    "r = (1 + 1/10)*B1/U_hat\n",
    "print(\"Relative increase in variance due to non-response\\n\",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare to the complete-data mean and covariance\n",
    "means_complete = r1\n",
    "var_complete = var1\n",
    "\n",
    "# We observe they are quite close! \n",
    "print(\"Difference between complete data and imputed data means:\\n\", means_complete - Q_hat)\n",
    "print(\"\\nDiference between complete data and imputed data variance:\\n\", np.abs(var_complete-T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Rubin's original rules assumed a normal distribution of the underlying values. Many quantities, such as means, standard deviations, regression coefficients and linear predictors follow this assumption. Other quantities however do not, and need to be tranformed towards normality. Van Buuren (2012) summarized such transformations per the literature:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Transformations estimators.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or is this the correct version!? nope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of all estimates for the artificial 10 datasets we built\n",
    "# More runs would give better estimates\n",
    "data_estimates = np.zeros((10,2))\n",
    "data_estimates_covar = np.zeros((10,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10): \n",
    "    X_imputed = clean_horse[['packed_cell_volume','pulse']]    \n",
    "    \n",
    "    # Fill missing values with random numbers but make sure they are positive.\n",
    "    # We make the assumption of independence between the two variables. \n",
    "    imputation_volume = np.abs(X_imputed['packed_cell_volume'].mean() - np.random.randint(0,30))\n",
    "    X_imputed['packed_cell_volume'] = X_imputed['packed_cell_volume'].fillna(imputation_volume)\n",
    "    \n",
    "    imputation_pulse = np.abs(X_imputed['pulse'].median()-np.random.randint(0,30))\n",
    "    X_imputed['pulse'] = X_imputed['pulse'].fillna(imputation_pulse)\n",
    "    \n",
    "    # The estimates \n",
    "    # Means of each variable\n",
    "    data_estimates[i,:] = [X_imputed.packed_cell_volume.mean(),X_imputed.pulse.mean()]\n",
    "    # Covariance of 2D dataset\n",
    "    # Covariance structure - (var(x,x), var(x,y), var(y,x),var(y,y))\n",
    "    data_estimates_covar[i,:] = np.cov(X_imputed.T)\n",
    "    \n",
    "\n",
    "# Inspect one mean and covariance Q_l and U_l\n",
    "print(\"Means: \\n\",data_estimates[1])\n",
    "print(\"\\nCovariance:\\n\",data_estimates_covar[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now apply Rubin's rules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Overall means \n",
    "Q_hat = data_estimates.mean(axis=0)\n",
    "print(\"Overall Means:\",Q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Covariance\n",
    "U_hat = data_estimates_covar.mean(axis=0)\n",
    "print(\"Overall mean covariance in datasets\\n\",U_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Variance in estimates - take 1\n",
    "diff = Q_hat-data_estimates\n",
    "B1 = 1/9 * np.dot(diff.T,diff)\n",
    "print(\"Take 1: Covariance within the estimates\\n\",B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check yourself before you wreck yourself\n",
    "#3. Variance in estimates - take 2 \n",
    "B2 = np.cov(data_estimates.T) \n",
    "print(\"Take 1: Covariance within the estimates\\n\",B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Total variance\n",
    "T = U_hat + (1+1/10)*B1\n",
    "print(\"Total Variance\\n\",T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Relative increase in variance due to nonresponse\n",
    "r = (1 + 1/10)*B1/U_hat\n",
    "print(\"Relative increase in variance due to non-response\\n\",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare to the complete-data mean and covariance\n",
    "means_complete = X_non_imputed.mean(axis=0) \n",
    "cov_complete = np.cov(X_non_imputed.T)\n",
    "\n",
    "# We observe they are quite close! \n",
    "print(\"Difference between complete data and imputed data means:\\n\", means_complete - Q_hat)\n",
    "print(\"\\nDiference between complete data and imputed data covariance:\\n\", cov_complete-T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More fun with missing data - Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many imputed datasets do you need?**\n",
    "\n",
    "While Rubin originally suggested that only a couple of imputed datasets would be sufficient (1987), more recent research indicates that significantly more imputations are needed. The appropriate number depends on the proportion of observations with missing values in the dataset. If you have 50% missing values for some variables for example, you will be looking at 40 imputed datasets. A comprehensive study on the desirable number of imputation for different scenarios was published by Graham (2007). He recommends “that one should use m=20, 20, 40, 100, and >100 for true γ= 0.10, 0.30, 0.50, 0.70, and 0.90, respectively.” (m = number of imputed datasets needed and γ = percentage of observations with missing data)\n",
    "\n",
    "**What variables should you include in your multiple imputation?**\n",
    "\n",
    "All of them. Rubin (1996) notes that the imputed dataset should include all the variables that will be used for subsequent analyses. This is one of the weaknesses of multiple imputation in practice: the person conducting the multiple imputation and generating the imputed datasets needs to be perfectly calibrated with the researchers conducting the analyses in terms of variables of interest. \n",
    "By the same logic, any interaction variables of interest also need to be included. If engineered variables are not included in the multiple imputation dataset, relationships that actually exist may not be found in subsequent analyses, as the imputations would have been generated with the assumption that those variables were independent.\n",
    "\n",
    "**Including the dependant variable?**\n",
    "\n",
    "Yes, including that one. This is not only recommended, but essential. For a full reasoning of why, see Allison (2001), Moons, Donders, Stijnen and Harrell (2006) and Graham (2009).\n",
    "\n",
    "**Should you include any other variables?**\n",
    "\n",
    "Yes. Collins et al. (2001) showed that you should include in your dataset, before imputation, not only the variables of interest but also all auxiliary variables. These are defined as variables that are not of direct interest in your study themselves, but which are correlated with the variables of interest in your study. Including them in your imputation approach was shown to reduce estimation bias, in particular as due to MNAR data.\n",
    "\n",
    "**If you have your healthy doubts about the MAR assumption in your dataset, should you still use these multiple imputation methods?**\n",
    "\n",
    "Yes. They are as good as anything we have, including when the MAR assumption is tenuous (Graham, 2009). It is actually Collins et al. (2001) who showed that MNAR data will not affect in a material way the internal validity of a study by itself. In the medical context, Mallinckrodt et al (2008) and O’Kelly and Ratitch (2014) both reached the same conclusion.\n",
    "\n",
    "**Should you/can you include categorical variables in your model?**\n",
    "\n",
    "Yes you should, and yes you can. However, pay attention here as different software implementations of the same algorithms require different preprocessing here (creating dummy variables or not, coding them as factors…).\n",
    "\n",
    "**Does multiple imputation work for small datasets with many variables?**\n",
    "\n",
    "Graham & Schafer (1999) showed that “MI performs very well in small samples (as low as N = 50), even with very large multiple regression models (as large as 18 predictors) and even with as much as 50% missing data in the DV.”\n",
    "\n",
    "**How can you conduct exploratory analysis on a complete dataset given the time that implementing MI requires?**\n",
    "\n",
    "At this level Allison (2001) offers a simple and elegant solution. When you generate the x imputed datasets that you will use in your multiple imputation procedure, simply generate x+1 instead. Use the extra dataset to conduct your exploratory analysis and evaluation your intended models for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data: roadmap\n",
    "\n",
    "1. Compute the amount of missing data per variable.\n",
    "This will tell you how large of a problem you have. In practical terms, it will also be an indication of how much effort you should expense on handling your missing data.\n",
    "\n",
    "2. Visualize your dataset itself with regards to missing data within it, or generate plots of the missing data across your dataset to check for patterns. If your dataset is longitudinal, check if the missing data is monotone.\n",
    "\n",
    "3. In complement or replacement to 3, compute in tables the missingness of different variables versus other variables including the response variable.\n",
    "\n",
    "4. As a follow up to 3, run a logistic regression of missingness for each variable with missing data as the response variable (yes/no), and the other variables as the explanatory variables.\n",
    "Steps 3 and 4 will allow you to check if any variables are correlated with the missingness in your study, and how.\n",
    "\n",
    "5. As a result of 3, 4, your investigation of the missing data in this study, and especially your discussion with data collectors and domain experts, decide if you can assume that your data is missing completely at random (MNAR) or at least missing at random (MAR).\n",
    "If you believe your data is MNAR, you can use Little’s test to confirm this. If your data is MCAR you know that basic methods of handling your missing data such as complete case analysis and available case analysis, will possibly significantly reduce the power of your study, but will not necessarily bias your results.\n",
    "If you can make the MNAR or MAR assumption, then your missing data is ignorable and you can proceed in confidence with a suitable multiple imputation method and then your intended statistical analyses.\n",
    "If your data is not missing at random – in this case you will still proceed with the same multiple imputation tools and analyses as above, but you will know that your results may be severely biased. As such, you will not use your results without at the very least complementing them with a suitable sensitivity analysis.\n",
    "\n",
    "6. Choose and run an appropriate multiple imputation procedure: maximum likelihood, multiple imputation, mice, hot deck or knn.\n",
    "\n",
    "7. Decide the number m of multiply imputer datasets you will need, per Graham’s guidelines (2007, 2009).\n",
    "\n",
    "8. Apply your analyses as you would with any other complete dataset.\n",
    "\n",
    "9. Pool your results following Rubin’s rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ODSC)",
   "language": "python",
   "name": "odsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
